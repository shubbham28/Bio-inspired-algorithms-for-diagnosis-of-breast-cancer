{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,math,copy,timeit\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy as dc\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n,dim):\n",
    "    gens=[[0 for g in range(dim)] for _ in range(n)]\n",
    "    for i,gen in enumerate(gens) :\n",
    "        r=random.randint(1,dim)\n",
    "        for _r in range(r):\n",
    "            gen[_r]=1\n",
    "        random.shuffle(gen)\n",
    "    return gens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_d,train_l,gen):\n",
    "        mask=np.array(gen) > 0\n",
    "        al_data=np.array([al[mask] for al in train_d])\n",
    "        kf = ms.KFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data):\n",
    "            s+= tree.DecisionTreeClassifier(max_depth=30, min_samples_split=20).fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])#.predict(al_test_data)\n",
    "        s/=4\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsig(n): return 1 / (1 + math.exp(-n))\n",
    "def sign(x): return 1 if x > 0 else (-1 if x!=0 else 0)\n",
    "\n",
    "def BPSO(train_d,train_l,n=20,max_iter=200,w1=0.5,c1=0.5,c2=0.5,vmax=4):\n",
    "    \"\"\"\n",
    "    input:{ \n",
    "            Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            max_iter: Number of max iteration, default=300\n",
    "            dim: Number of feature, default=None\n",
    "            prog: Do you want to use a progress bar?, default=False\n",
    "            w1: move rate, default=0.5\n",
    "            c1,c2: It's are two fixed variables, default=1,1\n",
    "            vmax: Limit search range of vmax, default=4\n",
    "            }\n",
    "    output:{\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] â†’ 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    dim=len(train_d[0])\n",
    "    personal_best=float(\"-inf\")\n",
    "    global_best=float(\"-inf\")\n",
    "    gens=random_search(n,dim)\n",
    "    vel=[[random.random()-0.5 for d in range(dim)] for _n in range(n)]\n",
    "    one_vel=[[random.random()-0.5 for d in range(dim)] for _n in range(n)]\n",
    "    zero_vel=[[random.random()-0.5 for d in range(dim)] for _n in range(n)]\n",
    "    fit=[float(\"-inf\") for i in range(n)]\n",
    "    personal_best=dc(fit)\n",
    "    xpersonal_best=dc(gens)\n",
    "    global_best=max(fit)\n",
    "    xglobal_best=gens[fit.index(max(fit))]\n",
    "    gens_dict={tuple([0]*dim):float(\"-inf\")}\n",
    "    for it in range(max_iter):\n",
    "        for i in range(n):\n",
    "            if tuple(gens[i]) in gens_dict:\n",
    "                score=gens_dict[tuple(gens[i])]\n",
    "            else:\n",
    "                score=evaluate(train_d,train_l,gens[i])\n",
    "                gens_dict[tuple(gens[i])]=score\n",
    "            fit[i]=score\n",
    "            if fit[i]>personal_best[i]:#max\n",
    "                personal_best[i]=dc(fit[i])\n",
    "                xpersonal_best[i]=dc(gens[i])\n",
    "        gg=max(fit)\n",
    "        xgg=gens[fit.index(gg)]\n",
    "        if global_best<gg:#max\n",
    "            global_best=dc(gg)\n",
    "            xglobal_best=dc(xgg)\n",
    "        oneadd=[[0 for d in range(dim)] for i in range(n)]\n",
    "        zeroadd=[[0 for d in range(dim)] for i in range(n)]\n",
    "        c3=c1*random.random()\n",
    "        dd3=c2*random.random()\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if xpersonal_best[i][j]==0:\n",
    "                    oneadd[i][j]=oneadd[i][j]-c3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]+c3\n",
    "                else:\n",
    "                    oneadd[i][j]=oneadd[i][j]+c3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]-c3\n",
    "                if xglobal_best[j]==0:\n",
    "                    oneadd[i][j]=oneadd[i][j]-dd3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]+dd3\n",
    "                else:\n",
    "                    oneadd[i][j]=oneadd[i][j]+dd3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]-dd3\n",
    "        one_vel=[[w1*_v+_a for _v,_a in zip(ov,oa)] for ov,oa in zip(one_vel,oneadd)]\n",
    "        zero_vel=[[w1*_v+_a for _v,_a in zip(ov,oa)] for ov,oa in zip(zero_vel,zeroadd)]\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if abs(vel[i][j]) > vmax:\n",
    "                    zero_vel[i][j]=vmax*sign(zero_vel[i][j])\n",
    "                    one_vel[i][j]=vmax*sign(one_vel[i][j])\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if gens[i][j]==1:\n",
    "                    vel[i][j]=zero_vel[i][j]\n",
    "                else:\n",
    "                    vel[i][j]=one_vel[i][j]\n",
    "        veln=[[logsig(s[_s]) for _s in range(len(s))] for s in vel]\n",
    "        temp=[[random.random() for d in range(dim)] for _n in range(n)]\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if temp[i][j]<veln[i][j]:\n",
    "                    gens[i][j]= 0 if gens[i][j] ==1 else 1\n",
    "                else:\n",
    "                    pass\n",
    "    return xglobal_best,xglobal_best.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(gen,tr_x,tr_y,te_x,te_y):\n",
    "    mask=np.array(gen) == 1\n",
    "    al_data=np.array(tr_x[:,mask])\n",
    "    al_test_data=np.array(te_x[:,mask])\n",
    "    return np.mean([tree.DecisionTreeClassifier(max_depth=30, min_samples_split=20).fit(al_data,tr_y).score(al_test_data,te_y) for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/shubbham28/Downloads/bc/wdbc1.csv\") as f:\n",
    "    x=np.array([[float(d) for d  in data.split(',')] for data in f.read().splitlines()])\n",
    "with open(\"/home/shubbham28/Downloads/bc/wdbc2.csv\") as f:\n",
    "    y=np.array([float(data) for data in f.read().splitlines()])\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y)\n",
    "train_d, test_d, train_l, test_l = train_test_split(x, training_scores_encoded, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9314685314685315\n",
      "1  001000000110001100001101010110  11  0.923077\n",
      "2  000111001110101110100100111110  17  0.949650\n",
      "3  000000111001000001100101001110  11  0.959441\n",
      "4  100100011111001000111101000100  14  0.932867\n",
      "5  100100111001000111100101000111  15  0.958042\n",
      "6  101101010010100101011101000110  15  0.928671\n",
      "7  001100011001001100000101001110  12  0.958042\n",
      "8  100000011001000100100101001101  11  0.965035\n",
      "9  001100111001001110000101000110  13  0.958042\n",
      "10  101100010001001001100101001100  12  0.958042\n",
      "11  000101110001001110100101000100  12  0.958042\n",
      "12  001000011001000101000101000110  10  0.958042\n",
      "13  101101110001001011000101010101  15  0.958042\n",
      "14  001001010001000010100101000100  9  0.965035\n",
      "15  100100111001000011100101010100  13  0.958042\n",
      "16  000001010001000001100101010111  11  0.960839\n",
      "17  001100011001001011100101001111  15  0.958042\n",
      "18  101101010001001011100101001110  15  0.958042\n",
      "19  000000010001101010111101000110  12  0.934266\n",
      "20  101101110001000110100101010101  15  0.958042\n",
      "Final:   001100010001001110100101000110   12   0.952867    2685.6226\n"
     ]
    }
   ],
   "source": [
    "k=[1 for r in range(len(x[0]))]\n",
    "print test_score(k,train_d,train_l,test_d,test_l)\n",
    "fattr=0\n",
    "ftest=0.0\n",
    "flist=[0 for r in range(len(x[0]))]\n",
    "final_list=[0 for r in range(len(x[0]))]\n",
    "start=timeit.default_timer()\n",
    "for i in range(20):\n",
    "    g,l=BPSO(train_d,train_l,n=20,max_iter=200,w1=0.5,c1=0.5,c2=0.5,vmax=4)\n",
    "    fattr+=l\n",
    "    test=test_score(g,train_d,train_l,test_d,test_l)\n",
    "    ftest+=test\n",
    "    for j in range(len(flist)):\n",
    "        if g[j]==1:\n",
    "            flist[j]+=1\n",
    "    print(\"{0}  {1}  {2}  {3:.6f}\".format(i+1,\"\".join(map(str,g)),l,test))\n",
    "fattr=fattr/20\n",
    "ftest=ftest/20\n",
    "end=timeit.default_timer()\n",
    "time=end-start\n",
    "final=np.argsort(flist)[::-1][:fattr]\n",
    "for i in range(len(final)):\n",
    "    final_list[final[i]]=1\n",
    "print(\"{0}  {1}   {2}   {3:.6f}    {4:.4f}\".format(\"Final: \",\"\".join(map(str,final_list)),fattr,ftest,time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
