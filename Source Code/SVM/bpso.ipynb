{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,math,copy,timeit\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy as dc\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n,dim):\n",
    "    gens=[[0 for g in range(dim)] for _ in range(n)]\n",
    "    for i,gen in enumerate(gens) :\n",
    "        r=random.randint(1,dim)\n",
    "        for _r in range(r):\n",
    "            gen[_r]=1\n",
    "        random.shuffle(gen)\n",
    "    return gens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_d,train_l,gen):\n",
    "        mask=np.array(gen) > 0\n",
    "        al_data=np.array([al[mask] for al in train_d])\n",
    "        kf = ms.KFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data):\n",
    "            s+= svm.LinearSVC().fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])#.predict(al_test_data)\n",
    "        s/=4\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsig(n): return 1 / (1 + math.exp(-n))\n",
    "def sign(x): return 1 if x > 0 else (-1 if x!=0 else 0)\n",
    "\n",
    "def BPSO(train_d,train_l,n=20,max_iter=200,w1=0.5,c1=0.5,c2=0.5,vmax=4):\n",
    "    \"\"\"\n",
    "    input:{ \n",
    "            Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            max_iter: Number of max iteration, default=300\n",
    "            dim: Number of feature, default=None\n",
    "            prog: Do you want to use a progress bar?, default=False\n",
    "            w1: move rate, default=0.5\n",
    "            c1,c2: It's are two fixed variables, default=1,1\n",
    "            vmax: Limit search range of vmax, default=4\n",
    "            }\n",
    "    output:{\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] â†’ 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    dim=len(train_d[0])\n",
    "    personal_best=float(\"-inf\")\n",
    "    global_best=float(\"-inf\")\n",
    "    gens=random_search(n,dim)\n",
    "    vel=[[random.random()-0.5 for d in range(dim)] for _n in range(n)]\n",
    "    one_vel=[[random.random()-0.5 for d in range(dim)] for _n in range(n)]\n",
    "    zero_vel=[[random.random()-0.5 for d in range(dim)] for _n in range(n)]\n",
    "    fit=[float(\"-inf\") for i in range(n)]\n",
    "    personal_best=dc(fit)\n",
    "    xpersonal_best=dc(gens)\n",
    "    global_best=max(fit)\n",
    "    xglobal_best=gens[fit.index(max(fit))]\n",
    "    gens_dict={tuple([0]*dim):float(\"-inf\")}\n",
    "    for it in range(max_iter):\n",
    "        for i in range(n):\n",
    "            if tuple(gens[i]) in gens_dict:\n",
    "                score=gens_dict[tuple(gens[i])]\n",
    "            else:\n",
    "                score=evaluate(train_d,train_l,gens[i])\n",
    "                gens_dict[tuple(gens[i])]=score\n",
    "            fit[i]=score\n",
    "            if fit[i]>personal_best[i]:#max\n",
    "                personal_best[i]=dc(fit[i])\n",
    "                xpersonal_best[i]=dc(gens[i])\n",
    "        gg=max(fit)\n",
    "        xgg=gens[fit.index(gg)]\n",
    "        if global_best<gg:#max\n",
    "            global_best=dc(gg)\n",
    "            xglobal_best=dc(xgg)\n",
    "        oneadd=[[0 for d in range(dim)] for i in range(n)]\n",
    "        zeroadd=[[0 for d in range(dim)] for i in range(n)]\n",
    "        c3=c1*random.random()\n",
    "        dd3=c2*random.random()\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if xpersonal_best[i][j]==0:\n",
    "                    oneadd[i][j]=oneadd[i][j]-c3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]+c3\n",
    "                else:\n",
    "                    oneadd[i][j]=oneadd[i][j]+c3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]-c3\n",
    "                if xglobal_best[j]==0:\n",
    "                    oneadd[i][j]=oneadd[i][j]-dd3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]+dd3\n",
    "                else:\n",
    "                    oneadd[i][j]=oneadd[i][j]+dd3\n",
    "                    zeroadd[i][j]=zeroadd[i][j]-dd3\n",
    "        one_vel=[[w1*_v+_a for _v,_a in zip(ov,oa)] for ov,oa in zip(one_vel,oneadd)]\n",
    "        zero_vel=[[w1*_v+_a for _v,_a in zip(ov,oa)] for ov,oa in zip(zero_vel,zeroadd)]\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if abs(vel[i][j]) > vmax:\n",
    "                    zero_vel[i][j]=vmax*sign(zero_vel[i][j])\n",
    "                    one_vel[i][j]=vmax*sign(one_vel[i][j])\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if gens[i][j]==1:\n",
    "                    vel[i][j]=zero_vel[i][j]\n",
    "                else:\n",
    "                    vel[i][j]=one_vel[i][j]\n",
    "        veln=[[logsig(s[_s]) for _s in range(len(s))] for s in vel]\n",
    "        temp=[[random.random() for d in range(dim)] for _n in range(n)]\n",
    "        for i in range(n):\n",
    "            for j in range(dim):\n",
    "                if temp[i][j]<veln[i][j]:\n",
    "                    gens[i][j]= 0 if gens[i][j] ==1 else 1\n",
    "                else:\n",
    "                    pass\n",
    "    return xglobal_best,xglobal_best.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(gen,tr_x,tr_y,te_x,te_y):\n",
    "    mask=np.array(gen) == 1\n",
    "    al_data=np.array(tr_x[:,mask])\n",
    "    al_test_data=np.array(te_x[:,mask])\n",
    "    return np.mean([svm.LinearSVC().fit(al_data,tr_y).score(al_test_data,te_y) for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/shubbham28/Downloads/bc/wdbc1.csv\") as f:\n",
    "    x=np.array([[float(d) for d  in data.split(',')] for data in f.read().splitlines()])\n",
    "with open(\"/home/shubbham28/Downloads/bc/wdbc2.csv\") as f:\n",
    "    y=np.array([float(data) for data in f.read().splitlines()])\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y)\n",
    "train_d, test_d, train_l, test_l = train_test_split(x, training_scores_encoded, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834965034965035\n",
      "1  110011100000000111011000111000  13  0.958042\n",
      "2  000001010011001010011100011011  13  0.969231\n",
      "3  000011000011101101101000101110  14  0.955245\n",
      "4  100001110101000001011000000110  11  0.955245\n",
      "5  000010011101001110111100111111  18  0.931469\n",
      "6  010010100110100110101000001100  12  0.958042\n",
      "7  000011110111001111011100101111  19  0.979021\n",
      "8  110001100001000100011000110101  12  0.965035\n",
      "9  000001111001000010001100101011  12  0.969231\n",
      "10  000000010101001000111100011011  12  0.967832\n",
      "11  010001000010001111101000101011  13  0.956643\n",
      "12  010000011000100111001000101010  11  0.965035\n",
      "13  010010100100001001111000011010  12  0.967832\n",
      "14  000011001111101000101100011110  15  0.959441\n",
      "15  010010100010100111001000011110  13  0.966434\n",
      "16  000011010010001110011100001101  13  0.962238\n",
      "17  000011001011000110001100111100  13  0.937063\n",
      "18  000001010111001011111100101001  15  0.958042\n",
      "19  000001101110000010011100011101  13  0.974825\n",
      "20  110000010001001001001000001111  11  0.972028\n",
      "Final:   000001010011001110011000001111   13   0.961399    10540.7647\n"
     ]
    }
   ],
   "source": [
    "k=[1 for r in range(len(x[0]))]\n",
    "print test_score(k,train_d,train_l,test_d,test_l)\n",
    "fattr=0\n",
    "ftest=0.0\n",
    "flist=[0 for r in range(len(x[0]))]\n",
    "final_list=[0 for r in range(len(x[0]))]\n",
    "start=timeit.default_timer()\n",
    "for i in range(20):\n",
    "    g,l=BPSO(train_d,train_l,n=20,max_iter=200,w1=0.5,c1=0.5,c2=0.5,vmax=4)\n",
    "    fattr+=l\n",
    "    test=test_score(g,train_d,train_l,test_d,test_l)\n",
    "    ftest+=test\n",
    "    for j in range(len(flist)):\n",
    "        if g[j]==1:\n",
    "            flist[j]+=1\n",
    "    print(\"{0}  {1}  {2}  {3:.6f}\".format(i+1,\"\".join(map(str,g)),l,test))\n",
    "fattr=fattr/20\n",
    "ftest=ftest/20\n",
    "end=timeit.default_timer()\n",
    "time=end-start\n",
    "final=np.argsort(flist)[::-1][:fattr]\n",
    "for i in range(len(final)):\n",
    "    final_list[final[i]]=1\n",
    "print(\"{0}  {1}   {2}   {3:.6f}    {4:.4f}\".format(\"Final: \",\"\".join(map(str,final_list)),fattr,ftest,time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
