{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,timeit\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy as dc\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n,dim):\n",
    "    gens=[[0 if g != j else 1 for g in range(n)] for j in range(dim)]\n",
    "    return gens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_d,train_l,gen):\n",
    "        mask=np.array(gen) > 0\n",
    "        al_data=np.array([al[mask] for al in train_d])\n",
    "        kf = ms.KFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data):\n",
    "            s+= svm.LinearSVC().fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])#.predict(al_test_data)\n",
    "        s/=4\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bees_optimization(bee,binary,i):\n",
    "    binary=list(binary)\n",
    "    j=random.randint(0,len(binary)-1)\n",
    "    k=random.randint(0,len(binary)-1)\n",
    "    while k==i:\n",
    "        k=random.randint(0,len(binary)-1)\n",
    "    fit=binary[j]+random.uniform(-1,1)*(binary[j]-binary[k])\n",
    "    for x in range(bee):\n",
    "        y=random.randint(0,len(binary)-1)\n",
    "        while y==i:\n",
    "            y=random.randint(0,len(binary)-1)\n",
    "        r=random.uniform(0,1)\n",
    "        if r<=fit:\n",
    "            binary[y]=1\n",
    "        \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BABCO(train_d,train_l,n=10,max_iter=25,employee_percent=0.5,max_limit=5):\n",
    "    \"\"\"\n",
    "    input:{ Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            max_iter: Number of max iteration, default=300\n",
    "            }\n",
    "    output:{\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] â†’ 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    employed_bees = int(round(n*employee_percent))\n",
    "    onlooker_bees = n - employed_bees       \n",
    "\n",
    "    dim=len(train_d[0])\n",
    "    global_best=float(\"-inf\")\n",
    "    global_position=tuple([0]*dim)\n",
    "    gens_dict = {}\n",
    "    limit=[0]*dim\n",
    "    gens=random_search(dim,dim)\n",
    "    for gen in gens:\n",
    "        if tuple(gen) in gens_dict:\n",
    "            score = gens_dict[tuple(gen)]\n",
    "        else:\n",
    "            score=evaluate(train_d,train_l,gen)\n",
    "            gens_dict[tuple(gen)]=score\n",
    "        if score > global_best:\n",
    "            global_best=score\n",
    "            global_position=dc(gen)\n",
    "            \n",
    "            \n",
    "    for it in range(max_iter):\n",
    "        for i in range(employed_bees):\n",
    "            for i,x in enumerate(gens):\n",
    "                gen=bees_optimization(employed_bees,x,i)\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "\n",
    "                if score > gens_dict[tuple(gens[i])]:\n",
    "                    limit[i]=0\n",
    "                    gens[i]= gen\n",
    "                else:\n",
    "                    limit[i]+=1\n",
    "\n",
    "                if score > global_best:\n",
    "                    global_best=score\n",
    "                    global_position=dc(gen)\n",
    "\n",
    "                if limit[i]>=max_limit:\n",
    "                    gens[i]=[0 if g != i else 1 for g in range(dim)]\n",
    "    \n",
    "        for i in range(onlooker_bees):\n",
    "            for i,x in enumerate(gens):\n",
    "                gen=bees_optimization(employed_bees,x,i)\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "\n",
    "                if score > gens_dict[tuple(gens[i])]:\n",
    "                    limit[i]=0\n",
    "                    gens[i]= gen\n",
    "                else:\n",
    "                    limit[i]+=1\n",
    "\n",
    "                if score > global_best:\n",
    "                    global_best=score\n",
    "                    global_position=dc(gen)\n",
    "\n",
    "                if limit[i]>=max_limit:\n",
    "                    gens[i]=[0 if g != i else 1 for g in range(dim)]\n",
    "\n",
    "                \n",
    "    return global_position,global_position.count(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(gen,tr_x,tr_y,te_x,te_y):\n",
    "    mask=np.array(gen) == 1\n",
    "    al_data=np.array(tr_x[:,mask])\n",
    "    al_test_data=np.array(te_x[:,mask])\n",
    "    return np.mean([svm.LinearSVC().fit(al_data,tr_y).score(al_test_data,te_y) for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/shubbham28/Downloads/bc/wdbc1.csv\") as f:\n",
    "    x=np.array([[float(d) for d  in data.split(',')] for data in f.read().splitlines()])\n",
    "with open(\"/home/shubbham28/Downloads/bc/wdbc2.csv\") as f:\n",
    "    y=np.array([float(data) for data in f.read().splitlines()])\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y)\n",
    "train_d, test_d, train_l, test_l = train_test_split(x, training_scores_encoded, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811188811188811\n",
      "1  110001100010001000001000001001  9  0.927273\n",
      "2  000001110001000101011000010110  11  0.972028\n",
      "3  010010100010100010101000001000  9  0.937063\n",
      "4  100011011001100111101000110011  16  0.935664\n",
      "5  100000100010001101011000100110  11  0.931469\n",
      "6  110001001001000001001000000110  9  0.923077\n",
      "7  100001100011000100110100010111  13  0.924476\n",
      "8  100000000000000000001000101000  4  0.948252\n",
      "9  000000000111100110011100110011  13  0.948252\n",
      "10  010000110011000100001000101011  11  0.941259\n",
      "11  010011101111000111110000111110  18  0.932867\n",
      "12  100011101111100111000100110001  16  0.951049\n",
      "13  000010110000101010011000001100  10  0.941259\n",
      "14  100000110011001111110100010011  15  0.931469\n",
      "15  000000101011100010111100000010  11  0.952448\n",
      "16  110000110101000010011000100101  12  0.937063\n",
      "17  010010100110001111101000101110  15  0.948252\n",
      "18  100001000000000010011000001000  6  0.944056\n",
      "19  000000000000100110001000011000  6  0.939860\n",
      "20  010001010011101000111000100101  13  0.932867\n",
      "Final:   100001100011000110011000100010   11   0.940000    635.7585\n"
     ]
    }
   ],
   "source": [
    "k=[1 for r in range(len(x[0]))]\n",
    "print test_score(k,train_d,train_l,test_d,test_l)\n",
    "fattr=0\n",
    "ftest=0.0\n",
    "flist=[0 for r in range(len(x[0]))]\n",
    "final_list=[0 for r in range(len(x[0]))]\n",
    "start=timeit.default_timer()\n",
    "for i in range(20):\n",
    "    g,l=BABCO(train_d,train_l,n=10,max_iter=25,employee_percent=0.5,max_limit=5)\n",
    "    fattr+=l\n",
    "    test=test_score(g,train_d,train_l,test_d,test_l)\n",
    "    ftest+=test\n",
    "    for j in range(len(flist)):\n",
    "        if g[j]==1:\n",
    "            flist[j]+=1\n",
    "    print(\"{0}  {1}  {2}  {3:.6f}\".format(i+1,\"\".join(map(str,g)),l,test))\n",
    "fattr=fattr/20\n",
    "ftest=ftest/20\n",
    "end=timeit.default_timer()\n",
    "time=end-start\n",
    "final=np.argsort(flist)[::-1][:fattr]\n",
    "for i in range(len(final)):\n",
    "    final_list[final[i]]=1\n",
    "print(\"{0}  {1}   {2}   {3:.6f}    {4:.4f}\".format(\"Final: \",\"\".join(map(str,final_list)),fattr,ftest,time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
